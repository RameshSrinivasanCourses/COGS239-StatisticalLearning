{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "* Classification, is an area of *supervised learning* that addresses the problem of how to systematically assign unlabeled (**classes** unknown) novel data to their labels (**classes** or groups or types) by using knowledge of their **features** (characteristics or attributes) that are obtained from observation and/or measurement.\n",
    "* A classifier algorithm is a specific technique or method for performing classification.\n",
    "* To learn to classify, the classifier algorithm first uses labeled (classes are known) training data to train a model (i.e., fit parameters), and then it uses a function known as its classification rule (or for short, the **classifier**) to assign a label to each new data point given the feature values.\n",
    "* A simple measure of classification performance is **accuracy**, that is what fraction of the test data is labeled accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automated checkout problem \n",
    "\n",
    "![](../images/pepperfeature.png)\n",
    "![](../images/peppertest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The intuitive importance of Classification\n",
    "* In conventional statistics courses, and experimental psychology or neuroscience courses, emphasis is placed on the notion of finding a \\textit{significant} difference between two (or more) subject groups or experimental conditions.  \n",
    "* For example in clinical research we ask questions such as - **Is the patient data different than the control data?**\n",
    "* But in our minds (and definitely in the patient and in the physicians mind) perhaps we should ask a different question - \n",
    "* Based on the characteristics of the patients data, can we determine if the data comes from a patient or a control?**\n",
    "* The first approach is built around hypothesis testing for differences, the second approach is classification of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Classifier?\n",
    "* At the simplest level a Classifier is a decision rule as in Signal Detection Theory (SDT), that allows us to categorize data. In fact, many of the ideas we will discussion closely mirror SDT.  \n",
    "* For example, when you go to the doctor they take your blood pressure, and you get a pair of numbers like 120/80 for the systolic/diastolic pressure. \n",
    "* The doctor has a decision rule.  If the systolic pressure is above 130, the patient receives a stern lecture about diet and exercise, and if the systolic pressure is above 140, medication is prescribed to lower blood pressure.  \n",
    "* Thus, there are 3 classes of patients based on the systolic blood pressure reading.\n",
    "1.  below 130 - healthy\n",
    "1.  130-140 - borderline \n",
    "1.  above 140 - medication \n",
    "* **This is a 3-class classifier** \n",
    "* How were these critical values found?  Hopefully, huge amounts of data are collected to look at patient cardiovascular health and blood pressure, and the data says if blood pressure remains above 140, the heart walls thicken and secondary cardiovascular diseases can emerge.   (There are other bad effects too).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples in Cognitive Science/Cognitive Neuroscience\n",
    "1. Categorization \n",
    "2. Automatic Speech Recognition\n",
    "3. Face Recognition\n",
    "4. Brain-Computer Interfaces\n",
    "5. Biomarkers for Mental Health \n",
    "6. Single-trial analysis of Neural Signals \n",
    "\n",
    "* In data science/machine learning applications, we are solely interested in making classifiers work as accurately as possible. \n",
    "* In scientific applications, we want to know how the classifier was able to work.  We want to know what features of the data were useful and what transformations of the data produced the accurate classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "* The first classifier we will discuss in this class is  **Logistic Regression**. \n",
    "* In Linear Regression, we fit a line to data. \n",
    "* In a simple (two-class) Logistic Regression we will fit a curve to the probability that the data comes from one **class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/Exam_pass_logistic_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Prediction Example \n",
    "[Pima Indians Diabetes Study](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "##NEW IMPORTS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima = pd.read_csv(\"../data/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I grabbed a list of all the columns \n",
    "cols = pima.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine how many of each outcome\n",
    "pima[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(pima,x = \"Pregnancies\",binwidth=1,hue = \"Outcome\",multiple=\"dodge\")\n",
    "plt.xticks(np.arange(1,18)-0.5,labels = range(1,18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into the predictors and Outcome Variable \n",
    "\n",
    "diabetes = pima['Outcome']\n",
    "predictors = pima[cols[1:8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations among predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(predictors.corr(), vmin=-1, vmax=1, cmap= \"jet\",annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pima, hue=\"Outcome\", height=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test sets\n",
    "* Here I made the decision to make the test size 25% of the data and training 75%\n",
    "* \n",
    " \n",
    "#predictors_train has the training data features \n",
    "#predictors_test has the testing data features\n",
    "#diabetes_train has the training data outcomes (targets)\n",
    "#diabetes_test has the testing data outcomes (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "predictors_train, predictors_test, diabetes_train, diabetes_test = train_test_split(predictors, diabetes, test_size=0.25,random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model (using the default parameters, except random_state and max_iter)\n",
    "logreg = LogisticRegression(random_state=16,max_iter = 5000)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(predictors_train, diabetes_train)\n",
    "\n",
    "diabetes_pred = logreg.predict(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see if the predictions match the expected outcomes \n",
    "correct = (diabetes_pred==diabetes_test)\n",
    "ncorrect = np.sum(correct)\n",
    "pctcorrect = 100*ncorrect/len(diabetes_test)\n",
    "print(pctcorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix \n",
    "\n",
    "A confusion matrix is a really nice way to summarize the performance of a classifer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(diabetes_test, diabetes_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Never say '\"Healthy\", \"Normal\", just say \"Undiagnosed\"\n",
    "class_names=['Undiagnosed','Diabetes'] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"jet\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.xticks(tick_marks+0.5, class_names)\n",
    "plt.yticks(tick_marks+0.5, class_names)\n",
    "plt.show()\n",
    "\n",
    "#Text(0.5,257.44,'Predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can also get this report \n",
    "print(metrics.classification_report(diabetes_test,diabetes_pred,target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision - What proportion of postive identifications were actually correct\n",
    "  $$ Precision = \\frac{TP}{TP+FP}$$ \n",
    "* TP = True Positive\n",
    "* FP = False Positive\n",
    "* TN - True Negative \n",
    "* FN - False Negative  \n",
    "* Recall - What proportion of actual positive was identified correctly? \n",
    "  $$ Recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Information \n",
    "\n",
    "* The strength of linear methods like logistic regression is that they can provide rich insight into the performance of the model. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_pprob = logreg.predict_proba(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(diabetes_pprob[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction probability is a confidence estimate on the prediction.  \n",
    "\n",
    "![](../images/PrecisionVsRecallBase.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(diabetes_pprob[:,1]),'ro')\n",
    "plt.xlabel('Ssamples')\n",
    "plt.ylabel('Probability of Diabetes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve \n",
    "* Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity.\n",
    "* An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "\n",
    "* True Positive Rate (TPR) is the same as recall in metrics and is therefore defined as follows:\n",
    "\n",
    "$$TPR = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "* False Positive Rate (FPR) is defined as follows:\n",
    "\n",
    "$$FPR = \\frac{FP}{FP+TN}$$\n",
    "\n",
    "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(diabetes_test,  diabetes_pprob[:,1])\n",
    "auc = metrics.roc_auc_score(diabetes_test, diabetes_pprob[:,1])\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.plot([0,1],[0,1],'r-')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The actual choice of threshold is somewhat arbitrary and depends on the importance of TPR and FPR for your classification problem. Typically, if there is no external concern (like death!), one option is to maximize TPR-FPR.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also (potentially) learn from these models which features were most useful in making the prediction by examining the coefficients of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.DataFrame(logreg.coef_,columns = cols[1:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The logistic regression model can be written as: \n",
    "\n",
    "$$\\hat{p}= \\dfrac{e^{w^T x}}{1+e^{w^T x}}$$\n",
    "\n",
    "$$\\hat{p}= \\dfrac{1}{1+e^{-w^T x}}$$\n",
    "\n",
    "where w are the weights that we can return and x are out features\n",
    "\n",
    "The decision boundaries are exactly at the position where the two classes are equiprobable. The boundary decision probability is exactly 0.5. Solving our sigmoid function for $p=0.5$:\n",
    "\n",
    "$$\\hat{p}= \\dfrac{1}{1+e^{-w^T x}} = 0.5 =  \\dfrac{1}{1+1} $$\n",
    "\n",
    "$$ e^{-w^T x} = 1$$\n",
    "\n",
    "$$ -w^T x = 0$$\n",
    "\n",
    "$$ w^T x = 0$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_train, predictors_test, diabetes_train, diabetes_test = train_test_split(predictors, diabetes, test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler() #This initializes the StandardScaler \n",
    "ss.fit(predictors_train)\n",
    "predictors_train = pd.DataFrame(ss.fit_transform(predictors_train), columns = predictors.columns)\n",
    "predictors_test = pd.DataFrame(ss.fit_transform(predictors_test),columns = predictors.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model (using the default parameters, escept random_state and max_iter)\n",
    "logreg = LogisticRegression(random_state=16,max_iter = 5000)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(predictors_train, diabetes_train)\n",
    "\n",
    "diabetes_pred = logreg.predict(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(diabetes_test,diabetes_pred,target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now I can examine the coefficients.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.DataFrame(logreg.coef_,columns = cols[1:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
